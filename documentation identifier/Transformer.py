

class Transformer(nn.Module):
    """    def __init__(
            self,
            src_vocab_size,
            trg_vocab_size,
            src_pad_idx,
            trg_pad_idx,
            embed_size=256,
            num_layers=6,
            forward_expansion=4,
            heads=8,
            dropout=0,
            device="cuda",
            max_length=100
    ):"""


#def make_src_mask(self, src):

#def make_trg_mask(self, trg):

#def forward(self, src, trg):
